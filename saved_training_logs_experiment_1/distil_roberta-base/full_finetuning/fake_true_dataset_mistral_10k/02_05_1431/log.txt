log_loss_steps: 208
eval_steps: 512
check_degradation: 512
Average Loss on fact answering task after 0 samples: 6.0821
Average Loss on fact answering task after 0 samples: 6.0401
Average Loss on fact answering task after 0 samples: 6.0629
Average Loss on fact answering task after 0 samples: 5.8909
Average Loss on fact answering task after 0 samples: 6.1072
----------------Epoch 1/1----------------
Epoch 1/1, Loss after 192 samples: 0.6988
Epoch 1/1, Loss after 400 samples: 0.6921
Average Loss on fact answering task after 496 samples: 5.6746
Average Loss on fact answering task after 496 samples: 5.8210
Average Loss on fact answering task after 496 samples: 5.9729
Average Loss on fact answering task after 496 samples: 5.8552
Average Loss on fact answering task after 496 samples: 5.9897
Mean accuracy: 0.6292, std: 0.0110, lower bound: 0.6080, upper bound: 0.6501 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 496 samples: 0.6288
Best model with eval accuracy 0.6288032454361054 with 496 samples seen is saved
Epoch 1/1, Loss after 608 samples: 0.6881
Epoch 1/1, Loss after 816 samples: 0.6457
Average Loss on fact answering task after 1008 samples: 6.2115
Average Loss on fact answering task after 1008 samples: 6.2326
Average Loss on fact answering task after 1008 samples: 6.2332
Average Loss on fact answering task after 1008 samples: 6.3267
Average Loss on fact answering task after 1008 samples: 6.3307
Mean accuracy: 0.5564, std: 0.0111, lower bound: 0.5340, upper bound: 0.5776 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1008 samples: 0.5563
Epoch 1/1, Loss after 1024 samples: 0.6314
Epoch 1/1, Loss after 1232 samples: 0.5148
Epoch 1/1, Loss after 1440 samples: 0.5377
Average Loss on fact answering task after 1520 samples: 6.6822
Average Loss on fact answering task after 1520 samples: 6.7269
Average Loss on fact answering task after 1520 samples: 6.6341
Average Loss on fact answering task after 1520 samples: 6.4010
Average Loss on fact answering task after 1520 samples: 6.6007
Mean accuracy: 0.8391, std: 0.0082, lower bound: 0.8220, upper bound: 0.8545 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1520 samples: 0.8392
Best model with eval accuracy 0.8392494929006086 with 1520 samples seen is saved
Epoch 1/1, Loss after 1648 samples: 0.4473
Epoch 1/1, Loss after 1856 samples: 0.3876
Average Loss on fact answering task after 2032 samples: 6.8611
Average Loss on fact answering task after 2032 samples: 6.7712
Average Loss on fact answering task after 2032 samples: 6.8187
Average Loss on fact answering task after 2032 samples: 6.9038
Average Loss on fact answering task after 2032 samples: 7.0367
Mean accuracy: 0.8724, std: 0.0075, lower bound: 0.8580, upper bound: 0.8859 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2032 samples: 0.8722
Best model with eval accuracy 0.8722109533468559 with 2032 samples seen is saved
Epoch 1/1, Loss after 2064 samples: 0.2865
Epoch 1/1, Loss after 2272 samples: 0.3990
Epoch 1/1, Loss after 2480 samples: 0.2595
Average Loss on fact answering task after 2544 samples: 6.8514
Average Loss on fact answering task after 2544 samples: 6.8346
Average Loss on fact answering task after 2544 samples: 6.8862
Average Loss on fact answering task after 2544 samples: 6.7516
Average Loss on fact answering task after 2544 samples: 6.9467
Mean accuracy: 0.8486, std: 0.0083, lower bound: 0.8311, upper bound: 0.8646 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2544 samples: 0.8484
Epoch 1/1, Loss after 2688 samples: 0.2734
Epoch 1/1, Loss after 2896 samples: 0.2387
Average Loss on fact answering task after 3056 samples: 6.8835
Average Loss on fact answering task after 3056 samples: 7.0066
Average Loss on fact answering task after 3056 samples: 6.9553
Average Loss on fact answering task after 3056 samples: 6.9477
Average Loss on fact answering task after 3056 samples: 7.0028
Mean accuracy: 0.7782, std: 0.0095, lower bound: 0.7596, upper bound: 0.7967 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3056 samples: 0.7784
Epoch 1/1, Loss after 3104 samples: 0.2412
Epoch 1/1, Loss after 3312 samples: 0.2402
Epoch 1/1, Loss after 3520 samples: 0.3682
Average Loss on fact answering task after 3568 samples: 6.8193
Average Loss on fact answering task after 3568 samples: 6.7137
Average Loss on fact answering task after 3568 samples: 6.7203
Average Loss on fact answering task after 3568 samples: 6.6030
Average Loss on fact answering task after 3568 samples: 6.8019
Mean accuracy: 0.9064, std: 0.0065, lower bound: 0.8935, upper bound: 0.9189 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3568 samples: 0.9062
Best model with eval accuracy 0.9061866125760649 with 3568 samples seen is saved
Epoch 1/1, Loss after 3728 samples: 0.2582
Epoch 1/1, Loss after 3936 samples: 0.2680
Average Loss on fact answering task after 4080 samples: 6.8919
Average Loss on fact answering task after 4080 samples: 6.9187
Average Loss on fact answering task after 4080 samples: 6.7990
Average Loss on fact answering task after 4080 samples: 6.7483
Average Loss on fact answering task after 4080 samples: 6.6971
Mean accuracy: 0.9201, std: 0.0063, lower bound: 0.9077, upper bound: 0.9320 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4080 samples: 0.9204
Best model with eval accuracy 0.9203853955375254 with 4080 samples seen is saved
Epoch 1/1, Loss after 4144 samples: 0.2662
Epoch 1/1, Loss after 4352 samples: 0.2588
Epoch 1/1, Loss after 4560 samples: 0.1752
Average Loss on fact answering task after 4592 samples: 7.1622
Average Loss on fact answering task after 4592 samples: 7.3316
Average Loss on fact answering task after 4592 samples: 7.1486
Average Loss on fact answering task after 4592 samples: 7.0426
Average Loss on fact answering task after 4592 samples: 7.2709
Mean accuracy: 0.8785, std: 0.0073, lower bound: 0.8641, upper bound: 0.8940 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4592 samples: 0.8783
Epoch 1/1, Loss after 4768 samples: 0.1602
Epoch 1/1, Loss after 4976 samples: 0.2311
Average Loss on fact answering task after 5104 samples: 7.1221
Average Loss on fact answering task after 5104 samples: 7.1207
Average Loss on fact answering task after 5104 samples: 7.1029
Average Loss on fact answering task after 5104 samples: 6.9527
Average Loss on fact answering task after 5104 samples: 7.2863
Mean accuracy: 0.9152, std: 0.0064, lower bound: 0.9026, upper bound: 0.9275 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5104 samples: 0.9153
Epoch 1/1, Loss after 5184 samples: 0.1590
Epoch 1/1, Loss after 5392 samples: 0.2165
Epoch 1/1, Loss after 5600 samples: 0.2178
Average Loss on fact answering task after 5616 samples: 7.1466
Average Loss on fact answering task after 5616 samples: 7.2046
Average Loss on fact answering task after 5616 samples: 7.2229
Average Loss on fact answering task after 5616 samples: 7.1546
Average Loss on fact answering task after 5616 samples: 7.1253
Mean accuracy: 0.9025, std: 0.0066, lower bound: 0.8884, upper bound: 0.9143 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5616 samples: 0.9026
Epoch 1/1, Loss after 5808 samples: 0.2181
Epoch 1/1, Loss after 6016 samples: 0.2138
Average Loss on fact answering task after 6128 samples: 7.1249
Average Loss on fact answering task after 6128 samples: 7.3055
Average Loss on fact answering task after 6128 samples: 7.2929
Average Loss on fact answering task after 6128 samples: 7.2275
Average Loss on fact answering task after 6128 samples: 7.1736
Mean accuracy: 0.9179, std: 0.0061, lower bound: 0.9047, upper bound: 0.9295 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6128 samples: 0.9178
Epoch 1/1, Loss after 6224 samples: 0.2697
Epoch 1/1, Loss after 6432 samples: 0.2097
Epoch 1/1, Loss after 6640 samples: 0.1524
Average Loss on fact answering task after 6640 samples: 7.4931
Average Loss on fact answering task after 6640 samples: 7.2939
Average Loss on fact answering task after 6640 samples: 7.1119
Average Loss on fact answering task after 6640 samples: 7.3334
Average Loss on fact answering task after 6640 samples: 7.4969
Mean accuracy: 0.8835, std: 0.0073, lower bound: 0.8697, upper bound: 0.8976 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6640 samples: 0.8834
Epoch 1/1, Loss after 6848 samples: 0.1463
Epoch 1/1, Loss after 7056 samples: 0.1451
Average Loss on fact answering task after 7152 samples: 7.4233
Average Loss on fact answering task after 7152 samples: 7.3836
Average Loss on fact answering task after 7152 samples: 7.2160
Average Loss on fact answering task after 7152 samples: 7.2658
Average Loss on fact answering task after 7152 samples: 7.2239
Mean accuracy: 0.9108, std: 0.0064, lower bound: 0.8981, upper bound: 0.9239 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7152 samples: 0.9108
Epoch 1/1, Loss after 7264 samples: 0.2690
Epoch 1/1, Loss after 7472 samples: 0.1449
Average Loss on fact answering task after 7664 samples: 7.2070
Average Loss on fact answering task after 7664 samples: 7.2570
Average Loss on fact answering task after 7664 samples: 7.2037
Average Loss on fact answering task after 7664 samples: 7.1216
Average Loss on fact answering task after 7664 samples: 7.0872
Mean accuracy: 0.9368, std: 0.0052, lower bound: 0.9270, upper bound: 0.9468 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7664 samples: 0.9371
Best model with eval accuracy 0.9371196754563894 with 7664 samples seen is saved
Epoch 1/1, Loss after 7680 samples: 0.1687
Epoch 1/1, Loss after 7888 samples: 0.1603
Epoch 1/1, Loss after 8096 samples: 0.1676
Average Loss on fact answering task after 8176 samples: 7.1782
Average Loss on fact answering task after 8176 samples: 6.8963
Average Loss on fact answering task after 8176 samples: 7.2985
Average Loss on fact answering task after 8176 samples: 6.9508
Average Loss on fact answering task after 8176 samples: 7.1512
Mean accuracy: 0.9406, std: 0.0053, lower bound: 0.9295, upper bound: 0.9508 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8176 samples: 0.9407
Best model with eval accuracy 0.9406693711967545 with 8176 samples seen is saved
Epoch 1/1, Loss after 8304 samples: 0.1888
Epoch 1/1, Loss after 8512 samples: 0.1102
Average Loss on fact answering task after 8688 samples: 7.3797
Average Loss on fact answering task after 8688 samples: 7.1741
Average Loss on fact answering task after 8688 samples: 7.3119
Average Loss on fact answering task after 8688 samples: 7.2898
Average Loss on fact answering task after 8688 samples: 7.3136
Mean accuracy: 0.8961, std: 0.0067, lower bound: 0.8834, upper bound: 0.9092 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8688 samples: 0.8960
Epoch 1/1, Loss after 8720 samples: 0.1603
Epoch 1/1, Loss after 8928 samples: 0.1887
Epoch 1/1, Loss after 9136 samples: 0.1322
Average Loss on fact answering task after 9200 samples: 7.1181
Average Loss on fact answering task after 9200 samples: 7.2806
Average Loss on fact answering task after 9200 samples: 7.4320
Average Loss on fact answering task after 9200 samples: 7.3819
Average Loss on fact answering task after 9200 samples: 7.1413
Mean accuracy: 0.9188, std: 0.0058, lower bound: 0.9077, upper bound: 0.9300 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9200 samples: 0.9189
Epoch 1/1, Loss after 9344 samples: 0.1018
Epoch 1/1, Loss after 9552 samples: 0.1994
Average Loss on fact answering task after 9712 samples: 7.1238
Average Loss on fact answering task after 9712 samples: 7.1367
Average Loss on fact answering task after 9712 samples: 7.3645
Average Loss on fact answering task after 9712 samples: 7.0724
Average Loss on fact answering task after 9712 samples: 7.1676
Mean accuracy: 0.9189, std: 0.0061, lower bound: 0.9072, upper bound: 0.9305 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9712 samples: 0.9189
Epoch 1/1, Loss after 9760 samples: 0.2272
Epoch 1/1, Loss after 9968 samples: 0.2196
Epoch 1/1, Loss after 10176 samples: 0.2040
Average Loss on fact answering task after 10224 samples: 7.2251
Average Loss on fact answering task after 10224 samples: 6.9230
Average Loss on fact answering task after 10224 samples: 7.1748
Average Loss on fact answering task after 10224 samples: 7.1145
Average Loss on fact answering task after 10224 samples: 7.0309
Mean accuracy: 0.8713, std: 0.0073, lower bound: 0.8560, upper bound: 0.8849 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10224 samples: 0.8707
Epoch 1/1, Loss after 10384 samples: 0.1472
Epoch 1/1, Loss after 10592 samples: 0.1362
Average Loss on fact answering task after 10736 samples: 7.1437
Average Loss on fact answering task after 10736 samples: 7.0760
Average Loss on fact answering task after 10736 samples: 7.1042
Average Loss on fact answering task after 10736 samples: 7.2963
Average Loss on fact answering task after 10736 samples: 7.0777
Mean accuracy: 0.9348, std: 0.0054, lower bound: 0.9249, upper bound: 0.9458 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10736 samples: 0.9346
Epoch 1/1, Loss after 10800 samples: 0.1491
Epoch 1/1, Loss after 11008 samples: 0.1527
Epoch 1/1, Loss after 11216 samples: 0.0978
Average Loss on fact answering task after 11248 samples: 7.0875
Average Loss on fact answering task after 11248 samples: 7.3428
Average Loss on fact answering task after 11248 samples: 7.1393
Average Loss on fact answering task after 11248 samples: 7.1490
Average Loss on fact answering task after 11248 samples: 7.1163
Mean accuracy: 0.9204, std: 0.0058, lower bound: 0.9092, upper bound: 0.9315 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11248 samples: 0.9199
Epoch 1/1, Loss after 11424 samples: 0.2085
Epoch 1/1, Loss after 11632 samples: 0.1015
Average Loss on fact answering task after 11760 samples: 7.1276
Average Loss on fact answering task after 11760 samples: 6.9261
Average Loss on fact answering task after 11760 samples: 7.0424
Average Loss on fact answering task after 11760 samples: 7.2235
Average Loss on fact answering task after 11760 samples: 7.0215
Mean accuracy: 0.8855, std: 0.0071, lower bound: 0.8707, upper bound: 0.8986 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11760 samples: 0.8854
Epoch 1/1, Loss after 11840 samples: 0.1342
Epoch 1/1, Loss after 12048 samples: 0.1635
Epoch 1/1, Loss after 12256 samples: 0.1424
Average Loss on fact answering task after 12272 samples: 7.2154
Average Loss on fact answering task after 12272 samples: 6.9903
Average Loss on fact answering task after 12272 samples: 7.2811
Average Loss on fact answering task after 12272 samples: 7.1930
Average Loss on fact answering task after 12272 samples: 7.0932
Mean accuracy: 0.8445, std: 0.0080, lower bound: 0.8291, upper bound: 0.8595 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12272 samples: 0.8443
Epoch 1/1, Loss after 12464 samples: 0.1298
Epoch 1/1, Loss after 12672 samples: 0.1572
Average Loss on fact answering task after 12784 samples: 6.9621
Average Loss on fact answering task after 12784 samples: 6.9361
Average Loss on fact answering task after 12784 samples: 6.9317
Average Loss on fact answering task after 12784 samples: 6.9761
Average Loss on fact answering task after 12784 samples: 7.0108
Mean accuracy: 0.9255, std: 0.0059, lower bound: 0.9138, upper bound: 0.9366 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12784 samples: 0.9260
Epoch 1/1, Loss after 12880 samples: 0.1559
Epoch 1/1, Loss after 13088 samples: 0.1129
Epoch 1/1, Loss after 13296 samples: 0.1699
Average Loss on fact answering task after 13296 samples: 7.1813
Average Loss on fact answering task after 13296 samples: 7.0919
Average Loss on fact answering task after 13296 samples: 6.9861
Average Loss on fact answering task after 13296 samples: 6.9408
Average Loss on fact answering task after 13296 samples: 6.9801
Mean accuracy: 0.9291, std: 0.0057, lower bound: 0.9178, upper bound: 0.9397 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13296 samples: 0.9290
Epoch 1/1, Loss after 13504 samples: 0.0924
Epoch 1/1, Loss after 13712 samples: 0.1540
Average Loss on fact answering task after 13808 samples: 7.1503
Average Loss on fact answering task after 13808 samples: 6.9040
Average Loss on fact answering task after 13808 samples: 7.0766
Average Loss on fact answering task after 13808 samples: 7.0476
Average Loss on fact answering task after 13808 samples: 7.3122
Mean accuracy: 0.8997, std: 0.0070, lower bound: 0.8859, upper bound: 0.9133 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13808 samples: 0.8996
Epoch 1/1, Loss after 13920 samples: 0.0743
Epoch 1/1, Loss after 14128 samples: 0.1595
Average Loss on fact answering task after 14320 samples: 6.9019
Average Loss on fact answering task after 14320 samples: 7.0516
Average Loss on fact answering task after 14320 samples: 7.1154
Average Loss on fact answering task after 14320 samples: 7.1904
Average Loss on fact answering task after 14320 samples: 7.0758
Mean accuracy: 0.9057, std: 0.0064, lower bound: 0.8925, upper bound: 0.9189 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14320 samples: 0.9057
Epoch 1/1, Loss after 14336 samples: 0.1284
Epoch 1/1, Loss after 14544 samples: 0.0999
Epoch 1/1, Loss after 14752 samples: 0.1727
Average Loss on fact answering task after 14832 samples: 6.8547
Average Loss on fact answering task after 14832 samples: 7.1569
Average Loss on fact answering task after 14832 samples: 7.0069
Average Loss on fact answering task after 14832 samples: 7.2268
Average Loss on fact answering task after 14832 samples: 6.9363
Mean accuracy: 0.8944, std: 0.0071, lower bound: 0.8803, upper bound: 0.9077 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14832 samples: 0.8945
Epoch 1/1, Loss after 14960 samples: 0.0863
Epoch 1/1, Loss after 15168 samples: 0.1280
Average Loss on fact answering task after 15344 samples: 7.0917
Average Loss on fact answering task after 15344 samples: 7.0926
Average Loss on fact answering task after 15344 samples: 7.1321
Average Loss on fact answering task after 15344 samples: 7.1066
Average Loss on fact answering task after 15344 samples: 6.9690
Mean accuracy: 0.9021, std: 0.0066, lower bound: 0.8889, upper bound: 0.9153 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15344 samples: 0.9021
Epoch 1/1, Loss after 15376 samples: 0.1594
Epoch 1/1, Loss after 15584 samples: 0.1186
Training signal is False, stopping training
Training finished
Best model: {'eval_acc': 0.9406693711967545, 'nb_samples': 8176}
Training loss logs: [{'samples': 192, 'loss': 0.6987750713641827}, {'samples': 400, 'loss': 0.6921128493088943}, {'samples': 608, 'loss': 0.6880868765024039}, {'samples': 816, 'loss': 0.6456815279447116}, {'samples': 1024, 'loss': 0.6313993013822116}, {'samples': 1232, 'loss': 0.5147834190955529}, {'samples': 1440, 'loss': 0.5377440819373498}, {'samples': 1648, 'loss': 0.44725740872896635}, {'samples': 1856, 'loss': 0.3876195320716271}, {'samples': 2064, 'loss': 0.28653797736534703}, {'samples': 2272, 'loss': 0.39896077376145583}, {'samples': 2480, 'loss': 0.2595243453979492}, {'samples': 2688, 'loss': 0.27342084737924427}, {'samples': 2896, 'loss': 0.23874557935274565}, {'samples': 3104, 'loss': 0.24121897037212664}, {'samples': 3312, 'loss': 0.2402129631776076}, {'samples': 3520, 'loss': 0.3681508486087506}, {'samples': 3728, 'loss': 0.258150183237516}, {'samples': 3936, 'loss': 0.2679672699708205}, {'samples': 4144, 'loss': 0.2661657608472384}, {'samples': 4352, 'loss': 0.25876696751667905}, {'samples': 4560, 'loss': 0.1752397234623249}, {'samples': 4768, 'loss': 0.16018532789670503}, {'samples': 4976, 'loss': 0.23113822020017183}, {'samples': 5184, 'loss': 0.15897585795475885}, {'samples': 5392, 'loss': 0.21650055509347182}, {'samples': 5600, 'loss': 0.2178274576480572}, {'samples': 5808, 'loss': 0.21808844346266526}, {'samples': 6016, 'loss': 0.2138361564049354}, {'samples': 6224, 'loss': 0.26967471837997437}, {'samples': 6432, 'loss': 0.2097352147102356}, {'samples': 6640, 'loss': 0.15242245564093956}, {'samples': 6848, 'loss': 0.14634167689543504}, {'samples': 7056, 'loss': 0.14509442448616028}, {'samples': 7264, 'loss': 0.2689561362449939}, {'samples': 7472, 'loss': 0.14494616710222685}, {'samples': 7680, 'loss': 0.16868083293621355}, {'samples': 7888, 'loss': 0.16026082405677208}, {'samples': 8096, 'loss': 0.16759119354761565}, {'samples': 8304, 'loss': 0.18880646045391375}, {'samples': 8512, 'loss': 0.11015436282524696}, {'samples': 8720, 'loss': 0.16025076921169573}, {'samples': 8928, 'loss': 0.1886795713351323}, {'samples': 9136, 'loss': 0.1322015363436479}, {'samples': 9344, 'loss': 0.10183145908208993}, {'samples': 9552, 'loss': 0.19943268024004424}, {'samples': 9760, 'loss': 0.22721829551916856}, {'samples': 9968, 'loss': 0.21957229880186227}, {'samples': 10176, 'loss': 0.20400808178461516}, {'samples': 10384, 'loss': 0.1472064462991861}, {'samples': 10592, 'loss': 0.1362407345038194}, {'samples': 10800, 'loss': 0.14913684129714966}, {'samples': 11008, 'loss': 0.15265784699183244}, {'samples': 11216, 'loss': 0.09777375367971566}, {'samples': 11424, 'loss': 0.208535380088366}, {'samples': 11632, 'loss': 0.10150503539122067}, {'samples': 11840, 'loss': 0.13417148016966307}, {'samples': 12048, 'loss': 0.163530537715325}, {'samples': 12256, 'loss': 0.14243219792842865}, {'samples': 12464, 'loss': 0.12982776417182043}, {'samples': 12672, 'loss': 0.1571966982804812}, {'samples': 12880, 'loss': 0.1559441192792012}, {'samples': 13088, 'loss': 0.11293369760880104}, {'samples': 13296, 'loss': 0.16990132859120002}, {'samples': 13504, 'loss': 0.09236721350596501}, {'samples': 13712, 'loss': 0.15399508865980002}, {'samples': 13920, 'loss': 0.07431022822856903}, {'samples': 14128, 'loss': 0.1595347420527385}, {'samples': 14336, 'loss': 0.12835717315857226}, {'samples': 14544, 'loss': 0.09988552217300121}, {'samples': 14752, 'loss': 0.1726896923321944}, {'samples': 14960, 'loss': 0.0863362206862523}, {'samples': 15168, 'loss': 0.12797869283419389}, {'samples': 15376, 'loss': 0.15936155101427665}, {'samples': 15584, 'loss': 0.1186008957716135}]
Evaluation accuracy logs: [{'samples': 496, 'accuracy': 0.6291678498985801, 'std': 0.011005809758861694, 'lower_bound': 0.6080121703853956, 'upper_bound': 0.6501140973630832}, {'samples': 1008, 'accuracy': 0.5564361054766734, 'std': 0.011140210024231703, 'lower_bound': 0.5339629817444219, 'upper_bound': 0.5775862068965517}, {'samples': 1520, 'accuracy': 0.8391115618661258, 'std': 0.008195674035916438, 'lower_bound': 0.8220081135902637, 'upper_bound': 0.8544624746450304}, {'samples': 2032, 'accuracy': 0.8724006085192698, 'std': 0.007465910592995351, 'lower_bound': 0.8580121703853956, 'upper_bound': 0.8859026369168357}, {'samples': 2544, 'accuracy': 0.8486075050709939, 'std': 0.008285015309160556, 'lower_bound': 0.8311359026369168, 'upper_bound': 0.8646171399594321}, {'samples': 3056, 'accuracy': 0.778183569979716, 'std': 0.009451411944032826, 'lower_bound': 0.7596348884381339, 'upper_bound': 0.7966531440162272}, {'samples': 3568, 'accuracy': 0.9064163286004057, 'std': 0.006490590744879848, 'lower_bound': 0.8934964503042596, 'upper_bound': 0.9188640973630832}, {'samples': 4080, 'accuracy': 0.9201029411764706, 'std': 0.006274783836980515, 'lower_bound': 0.907707910750507, 'upper_bound': 0.9320486815415822}, {'samples': 4592, 'accuracy': 0.8784959432048681, 'std': 0.007345956744355519, 'lower_bound': 0.8640973630831643, 'upper_bound': 0.8940162271805274}, {'samples': 5104, 'accuracy': 0.9152134888438134, 'std': 0.006352953623222675, 'lower_bound': 0.9026242393509127, 'upper_bound': 0.9274847870182555}, {'samples': 5616, 'accuracy': 0.9024695740365112, 'std': 0.006567575368202329, 'lower_bound': 0.8884254563894523, 'upper_bound': 0.9143128803245436}, {'samples': 6128, 'accuracy': 0.9178559837728195, 'std': 0.006121255882966371, 'lower_bound': 0.9046653144016227, 'upper_bound': 0.9295131845841785}, {'samples': 6640, 'accuracy': 0.8834721095334686, 'std': 0.0073432150649042184, 'lower_bound': 0.8696754563894523, 'upper_bound': 0.8975786004056795}, {'samples': 7152, 'accuracy': 0.9107961460446247, 'std': 0.006375590665879142, 'lower_bound': 0.8980730223123732, 'upper_bound': 0.9239477687626775}, {'samples': 7664, 'accuracy': 0.9368159229208924, 'std': 0.0051542905011948996, 'lower_bound': 0.9269776876267748, 'upper_bound': 0.9467545638945233}, {'samples': 8176, 'accuracy': 0.9405552738336713, 'std': 0.005334781401703399, 'lower_bound': 0.9295131845841785, 'upper_bound': 0.9508113590263692}, {'samples': 8688, 'accuracy': 0.896093813387424, 'std': 0.006670228780404273, 'lower_bound': 0.8833671399594321, 'upper_bound': 0.9092292089249493}, {'samples': 9200, 'accuracy': 0.9188397565922921, 'std': 0.005808705573929679, 'lower_bound': 0.907707910750507, 'upper_bound': 0.9300329614604462}, {'samples': 9712, 'accuracy': 0.918870182555781, 'std': 0.006066864216282457, 'lower_bound': 0.9072008113590264, 'upper_bound': 0.9305273833671399}, {'samples': 10224, 'accuracy': 0.8713260649087222, 'std': 0.00733876325982051, 'lower_bound': 0.8559837728194726, 'upper_bound': 0.8848884381338742}, {'samples': 10736, 'accuracy': 0.934830121703854, 'std': 0.00542311386091256, 'lower_bound': 0.9249492900608519, 'upper_bound': 0.945753042596349}, {'samples': 11248, 'accuracy': 0.9204447261663286, 'std': 0.005848360177815487, 'lower_bound': 0.9092165314401622, 'upper_bound': 0.9315415821501014}, {'samples': 11760, 'accuracy': 0.88551369168357, 'std': 0.007130703027848424, 'lower_bound': 0.8706896551724138, 'upper_bound': 0.898592799188641}, {'samples': 12272, 'accuracy': 0.8445334685598377, 'std': 0.007954603576865508, 'lower_bound': 0.829107505070994, 'upper_bound': 0.8595461460446248}, {'samples': 12784, 'accuracy': 0.9255436105476674, 'std': 0.005906443639803634, 'lower_bound': 0.9137931034482759, 'upper_bound': 0.9366125760649088}, {'samples': 13296, 'accuracy': 0.9291450304259634, 'std': 0.0056667971056583565, 'lower_bound': 0.9178498985801217, 'upper_bound': 0.9396551724137931}, {'samples': 13808, 'accuracy': 0.8996774847870184, 'std': 0.006990969834077396, 'lower_bound': 0.8859026369168357, 'upper_bound': 0.9132986815415822}, {'samples': 14320, 'accuracy': 0.9057332657200812, 'std': 0.006422985110350426, 'lower_bound': 0.8924822515212981, 'upper_bound': 0.9188640973630832}, {'samples': 14832, 'accuracy': 0.8943595334685598, 'std': 0.007084646216488441, 'lower_bound': 0.8803245436105477, 'upper_bound': 0.907707910750507}, {'samples': 15344, 'accuracy': 0.9021333671399593, 'std': 0.006552127430456524, 'lower_bound': 0.888932555780933, 'upper_bound': 0.915314401622718}]
